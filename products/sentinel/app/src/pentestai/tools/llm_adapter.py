import requests
import logging

logger = logging.getLogger(__name__)

class BaseLLM:
    def generate(self, messages, **kwargs) -> str:
        raise NotImplementedError

class OllamaLLM(BaseLLM):
    def __init__(self, url: str, model: str):
        self.url = url.rstrip("/")
        self.model = model
        logger.info(f"OllamaLLM initialized with URL: {self.url}, model: {self.model}")

    def generate(self, messages, **kwargs) -> str:
        try:
            r = requests.post(
                f"{self.url}/api/chat",
                json={"model": self.model, "messages": messages, "stream": False},
                timeout=120,
            )
            r.raise_for_status()
            data = r.json()
            # Ollama returns {"message":{"content": "..."}}
            content = data.get("message", {}).get("content") or data.get("response")
            return content or ""
        except requests.exceptions.RequestException as e:
            logger.error(f"Ollama request failed: {e}")
            return f"[Ollama error: {e}]"

class OpenAIAdapter(BaseLLM):
    def __init__(self, api_base: str, api_key: str, model: str):
        self.api_base = api_base.rstrip("/")
        self.api_key = api_key
        self.model = model

    def generate(self, messages, **kwargs) -> str:
        try:
            r = requests.post(
                f"{self.api_base}/chat/completions",
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={"model": self.model, "messages": messages},
                timeout=120,
            )
            r.raise_for_status()
            return r.json()["choices"][0]["message"]["content"]
        except requests.exceptions.RequestException as e:
            logger.error(f"OpenAI API request failed: {e}")
            return f"[OpenAI error: {e}]"
