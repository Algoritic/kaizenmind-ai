from __future__ import annotations

import argparse
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path

from pentestai.langgraph_graphs.hybrid_review_graph import build_graph
# NOTE: `load_runtime_config` should return a fully-populated RuntimeConfig,
# including .llm, .logging, .artifacts_workdir, .timeout_seconds, .use_langgraph
from pentestai.config import load_runtime_config, setup_logging


logger = logging.getLogger(__name__)


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description="Run PentestAI LangGraph-based hybrid review and write reports/"
    )
    p.add_argument(
        "--repo",
        type=str,
        help="Path to a local source repository to analyze (optional if --apk provided)",
    )
    p.add_argument(
        "--apk",
        type=str,
        help="Path to an APK to analyze (optional if --repo provided)",
    )
    p.add_argument(
        "--out",
        type=str,
        default="/app/reports",
        help="Output directory for reports (JSON/Markdown). Default: /app/reports",
    )
    # logging overrides (propagate into runtime config at load time)
    p.add_argument("--log-file", default=None, help="Override PENTESTAI_LOG_FILE")
    p.add_argument("--log-level", default=None, help="Override PENTESTAI_LOG_LEVEL")
    return p.parse_args()


def _validate_inputs(repo_path: str | None, apk_path: str | None) -> dict:
    """
    Validate input paths and build initial state.
    """
    state: dict = {}

    if not repo_path and not apk_path:
        logger.error("You must provide at least one of --repo or --apk.")
        sys.exit(2)

    if repo_path:
        abs_repo = os.path.abspath(repo_path)
        if not os.path.exists(abs_repo):
            logger.error("Repository path does not exist: %s", abs_repo)
            sys.exit(2)
        state["repo_path"] = abs_repo

    if apk_path:
        abs_apk = os.path.abspath(apk_path)
        if not os.path.exists(abs_apk):
            logger.error("APK path does not exist: %s", abs_apk)
            sys.exit(2)
        state["apk_path"] = abs_apk

    return state


def _init_llm(cfg, log: logging.Logger):
    """
    Initialize the LLM from cfg.llm and return a LangChain-compatible chat model,
    or None if not configured. Exits on hard failures (e.g., unreachable Ollama).
    """
    llm = None
    backend = getattr(cfg.llm, "backend", None)

    if backend == "ollama":
        try:
            from langchain_ollama import ChatOllama
        except Exception as e:
            log.error("Failed to import ChatOllama (langchain_community): %s", e)
            sys.exit(1)

        model = getattr(cfg.llm, "ollama_model", None) or os.getenv("OLLAMA_MODEL", "phi3:mini")
        base_url = getattr(cfg.llm, "ollama_url", None) or os.getenv("OLLAMA_URL", "http://host.docker.internal:11434")

        llm = ChatOllama(model=model, base_url=base_url)

        # Optional connectivity test for Ollama (recommended, exits on failure)
        try:
            from pentestai.tools.ollama_test import test_ollama_connection
            if not test_ollama_connection(llm):
                log.error("Ollama connection test failed (base_url=%s, model=%s).", base_url, model)
                sys.exit(1)
            log.info("Ollama connection OK (base_url=%s, model=%s).", base_url, model)
        except Exception as e:
            log.warning("Skipping Ollama connectivity test (not available?): %s", e)

    elif backend == "openai":
        try:
            from langchain_openai import ChatOpenAI
        except Exception as e:
            log.error("Failed to import ChatOpenAI (langchain_openai): %s", e)
            sys.exit(1)

        api_key = getattr(cfg.llm, "openai_api_key", None) or os.getenv("OPENAI_API_KEY")
        api_base = getattr(cfg.llm, "openai_api_base", None) or os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        model = getattr(cfg.llm, "openai_model", None) or os.getenv("OPENAI_MODEL", "gpt-4o-mini")

        if not api_key:
            log.error("OPENAI-style backend selected but no API key provided.")
            sys.exit(1)

        llm = ChatOpenAI(api_key=api_key, base_url=api_base, model=model)
        log.info("OpenAI-compatible LLM initialized (base=%s, model=%s).", api_base, model)

    else:
        log.warning("No/unknown LLM backend configured (cfg.llm.backend=%r). Persona agents may not run.", backend)

    return llm


def _write_reports(out_dir: str, final_report: dict, run_id: str) -> tuple[str, str]:
    """
    Write JSON and Markdown reports. Returns (json_path, md_path).
    """
    out_path = Path(out_dir)
    out_path.mkdir(parents=True, exist_ok=True)

    # JSON
    json_path = out_path / "final_report.json"
    with json_path.open("w", encoding="utf-8") as f:
        json.dump(final_report, f, indent=2)

    # Markdown (human-readable)
    md_path = out_path / "final_report.md"

    summary = final_report.get("summary", {}) or {}
    findings = final_report.get("findings_ranked", []) or []

    # Make a readable Markdown — no escaped newlines, include a nice summary table if possible.
    lines: list[str] = []
    lines.append("# Security AI — Hybrid Review Report")
    lines.append(f"**Run ID:** {run_id}")
    lines.append(f"**Generated:** {datetime.utcnow().isoformat()}Z")
    lines.append("")

    # Summary: render as a table and as JSON (for details)
    if summary:
        # Table
        keys = ["critical", "high", "medium", "low"]
        lines.append("## Summary")
        lines.append("")
        lines.append("| critical | high | medium | low |")
        lines.append("|---------:|-----:|-------:|----:|")
        lines.append(
            f"| {summary.get('critical', 0)} | {summary.get('high', 0)} | {summary.get('medium', 0)} | {summary.get('low', 0)} |"
        )
        lines.append("")
        lines.append("<details><summary>Summary JSON</summary>")
        lines.append("")
        lines.append("```json")
        lines.append(json.dumps(summary, indent=2))
        lines.append("```")
        lines.append("</details>")
        lines.append("")
    else:
        lines.append("## Summary")
        lines.append("_No summary provided._")
        lines.append("")

    # Findings
    lines.append("## Ranked Findings")
    if findings:
        for item in findings:
            sev = str(item.get("severity", "")).lower()
            title = item.get("title", "(untitled finding)")
            lines.append(f"- **[{sev}]** {title}")
    else:
        lines.append("_No findings reported._")

    # Optional per-persona sections if present
    sections = final_report.get("sections") or []
    if sections:
        lines.append("")
        lines.append("## Persona Sections")
        for sec in sections:
            persona = sec.get("persona", "(unknown persona)")
            lines.append(f"### {persona}")
            # Prefer markdown content if provided; otherwise dump JSON
            md = sec.get("markdown")
            if md:
                lines.append(md.strip())
            else:
                lines.append("```json")
                lines.append(json.dumps(sec, indent=2))
                lines.append("```")

    with md_path.open("w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    return str(json_path), str(md_path)


def main() -> None:
    args = parse_args()

    # Load runtime config (honors overrides)
    cfg = load_runtime_config(log_file=args.log_file, log_level=args.log_level)

    # Initialize logging with the loaded config
    setup_logging(cfg.logging)
    log = logging.getLogger("pentestai.cli")

    log.info("Effective logging: level=%s file=%s", getattr(cfg.logging, "level", None), getattr(cfg.logging, "file", None))
    log.info(
        "Runtime: use_langgraph=%s, artifacts=%s, timeout_seconds=%s",
        getattr(cfg, "use_langgraph", None),
        getattr(cfg, "artifacts_workdir", None),
        getattr(cfg, "timeout_seconds", None),
    )
    log.info("LLM backend=%s", getattr(cfg.llm, "backend", None))

    # Validate inputs and build initial state
    state = _validate_inputs(args.repo, args.apk)

    # Initialize LLM (if configured) and attach to state
    llm = _init_llm(cfg, log)
    if llm is not None:
        state["llm"] = llm
    else:
        log.warning("Proceeding without an LLM; external persona agents may be skipped or stubbed.")

    # Enforce LangGraph usage if required
    if not getattr(cfg, "use_langgraph", False):
        log.error("use_langgraph is disabled in the runtime config. Enable it to run the full persona pipeline.")
        sys.exit(1)

    log.info("Building LangGraph app...")
    try:
        app = build_graph()
    except Exception as e:
        log.exception("Failed to build LangGraph graph: %s", e)
        sys.exit(1)

    if not app:
        log.error("LangGraph is not available (build_graph() returned None). Ensure 'langgraph' is installed in this runtime.")
        sys.exit(1)

    log.info("Invoking LangGraph...")
    try:
        state = app.invoke(state)
    except Exception as e:
        log.exception("LangGraph invocation failed: %s", e)
        sys.exit(1)

    log.info("LangGraph invocation complete.")

    final_report = state.get("final_report")
    if not final_report:
        log.error("Final report not found in state; the pipeline may have short-circuited.")
        sys.exit(1)

    # Prepare outputs
    run_id = final_report.get("id") or f"run-{datetime.utcnow().isoformat()}Z"
    json_path, md_path = _write_reports(args.out, final_report, run_id)

    print(f"Wrote: {json_path}")
    print(f"Wrote: {md_path}")


if __name__ == "__main__":
    main()
