from dataclasses import dataclass
from typing import List, Dict, Any
from pentestai.tools import static_analyzers, test_runner, emulator, ghidra, pentest_tools
from pentestai.tools.llm_adapter import BaseLLM
from pentestai.utils.security import redact
import logging

logger = logging.getLogger(__name__)

@dataclass
class PersonaResult:
    name: str
    findings: List[Dict[str, Any]]
    artifacts: List[str]

class PlannerPersona:
    def __init__(self, llm: BaseLLM): self.llm = llm
    def plan(self, repo_path: str | None, apk_path: str | None, targets: list[str] | None) -> Dict[str, Any]:
        logger.info(f"PlannerPersona planning for repo: {repo_path}, apk: {apk_path}, targets: {targets}")
        tasks = []
        if repo_path:
            tasks += ["static","tests"]
        if apk_path:
            tasks += ["ghidra","android_dynamic"]
        if targets:
            tasks += ["pentest"]
        logger.info(f"PlannerPersona planned tasks: {tasks}")
        return {"tasks": tasks, "notes": "Auto-planned"}

class CodeReviewerPersona:
    def __init__(self, llm: BaseLLM): self.llm = llm
    def run(self, repo_path: str) -> PersonaResult:
        logger.info(f"CodeReviewerPersona running for repo: {repo_path}")
        tools_out = static_analyzers.run_all(repo_path)
        
        # --- ENHANCED LLM PROMPT FOR SRE, QUALITY, AND FAILURE ANALYSIS ---
        sys_prompt = (
            "You are a Senior System Architect and Lead Code Reviewer, specializing in Security and SRE best practices. "
            "Analyze the static analysis tool output (Bandit, Semgrep, ESLint) and the inferred codebase quality. "
            "Generate a structured markdown report that includes the following sections:\n\n"
            "## 📐 Code Quality & Engineering Practices\n"
            "Evaluate the overall quality, maintainability, modularity, and adherence to standard coding practices. "
            "Point out issues like excessive complexity, poor abstraction, or redundant code.\n\n"
            "## ⚙️ SRE & Observability Review\n"
            "Check for critical SRE practices based on the code structure and configuration inferred from the scan results. "
            "Identify missing or poor implementation of logging, metrics, alerting hooks, error handling logic, and resource management. "
            "Identify at least one key SRE risk finding with a 'high' severity.\n\n"
            "## 📉 Code Tendency to Fail\n"
            "Based on complexity metrics, unhandled exceptions, and reported static analysis issues (e.g., resource management problems), "
            "identify the top 3 specific functions or code portions that have the highest tendency to fail in production or under stress. "
            "For the most critical one, provide a code-level fix or recommendation."
        )

        msgs = [
            {"role":"system","content": sys_prompt}, 
            {"role":"user","content": redact(str(tools_out))}
        ]
        
        summary = self.llm.generate(msgs)
        logger.info(f"CodeReviewerPersona generated summary: {summary[:100]}...")
        
        # The LLM's full analysis is returned as a single detailed finding
        return PersonaResult("CodeReviewer", [{
            "id":"CR-QSA",
            "title":"Code Quality, SRE, and Stability Analysis",
            "severity":"medium", # Default to medium, let the LLM output highlight specific high risks
            "description":summary,
            "details": "LLM-generated detailed code quality and SRE analysis."
        }], [])

class UnitTestWriterPersona:
    def __init__(self, llm: BaseLLM): self.llm = llm
    def run(self, repo_path: str) -> PersonaResult:
        logger.info(f"UnitTestWriterPersona running for repo: {repo_path}")
        results = {}
        for fn in (test_runner.pytest_run, test_runner.gradle_test, test_runner.mvn_test):
            try:
                results[fn.__name__] = fn(repo_path)
            except Exception as e:
                logger.error(f"Error running {fn.__name__}: {e}")
                results[fn.__name__] = {"error": str(e)}
        
        # This prompt was updated in the previous step
        sys_prompt = (
            "You are a highly detailed Test Engineer and QA specialist. "
            "Your task is to review the results of unit test execution and codebase structure. "
            "Generate a structured markdown report that includes the following sections:\n\n"
            "## 🧪 Unit Test Coverage & Gaps\n"
            "Summarize the existing test results (failures, passes). If test coverage is low or non-existent, "
            "provide a complete, runnable code example of a *missing unit test* for a key function as a finding.\n\n"
            "## 💥 Potential Points of Breakdown and Failure\n"
            "Analyze the codebase (inferred from tool results) and point out high-risk areas "
            "where the application is most likely to break, focusing on unhandled edge cases, "
            "resource leaks, or concurrency issues. Classify a finding as 'high' severity for the worst point of failure.\n\n"
            "## 📈 Change and Impact Analysis\n"
            "Based on the repository's complexity and test results, provide a brief analysis of the "
            "potential impact of a significant code change. Highlight the subsystems that would require "
            "the most regression testing."
        )

        msgs = [
            {"role":"system","content": sys_prompt}, 
            {"role": "user", "content": redact(str(results))}
        ]
        
        summary = self.llm.generate(msgs)
        logger.info(f"UnitTestWriterPersona generated summary: {summary[:100]}...")
        
        # The LLM's full analysis is returned as a single detailed finding
        return PersonaResult("UnitTestWriter", [{
            "id":"UT-DETAIL",
            "title":"Unit Test, Stability, and Impact Analysis",
            "severity":"medium", 
            "description": summary,
            "details": "LLM-generated detailed stability analysis."
        }], [])

class MobileSecurityAnalystPersona:
    def __init__(self, llm: BaseLLM): self.llm = llm
    def run(self, apk_path: str) -> PersonaResult:
        logger.info(f"MobileSecurityAnalystPersona running for apk: {apk_path}")
        res = emulator.android_install_and_monkey(apk_path)
        msgs = [{"role":"system","content":"Analyze Android runtime logs for security issues."}, 
                {"role":"user","content": redact(str(res))}]
        summary = self.llm.generate(msgs)
        logger.info(f"MobileSecurityAnalystPersona generated summary: {summary[:100]}...")
        return PersonaResult("MobileSecurityAnalyst", [{"id":"ANDR-DYN","title":"Android dynamic checks","severity":"medium","description":summary}], [])

class GhidraScripterPersona:
    def __init__(self, llm: BaseLLM): self.llm = llm
    def run(self, binary_path: str) -> PersonaResult:
        logger.info(f"GhidraScripterPersona running for binary: {binary_path}")
        out = ghidra.run_headless(binary_path, ["strings SSL/MD5/AES", "JNI/native calls"])
        msgs = [{"role":"system","content":"Summarize risky functions and weak crypto."}, 
                {"role": "user", "content": redact(str(out))}]
        summary = self.llm.generate(msgs)
        logger.info(f"GhidraScripterPersona generated summary: {summary[:100]}...")
        return PersonaResult("GhidraScripter", [{"id":"GHIDRA","title":"Binary analysis summary","severity":"medium","description":summary}], [])

class PentesterPersona:
    def __init__(self, llm: BaseLLM): self.llm = llm
    def run(self, targets: list[str]) -> PersonaResult:
        logger.info(f"PentesterPersona running for targets: {targets}")
        res = {}
        for t in targets or []:
            try:
                res[t] = pentest_tools.nmap_scan(t)
            except Exception as e:
                logger.error(f"Error running nmap_scan for {t}: {e}")
                res[t] = {"error": str(e)}
        msgs = [{"role":"system","content":"Summarize network recon and propose attack paths."}, 
                {"role": "user", "content": redact(str(res))}]
        summary = self.llm.generate(msgs)
        logger.info(f"PentesterPersona generated summary: {summary[:100]}...")
        return PersonaResult("Pentester", [{"id":"PTEST","title":"Pentest reconnaissance","severity":"low","description":summary}], [])